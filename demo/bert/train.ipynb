{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート\n",
    "\n",
    "import numpy as np  # 数値計算用ライブラリ\n",
    "import pandas as pd  # データ操作用ライブラリ\n",
    "import torch  # PyTorchのインポート\n",
    "from sklearn.model_selection import train_test_split  # データ分割のための関数\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    "    Dataset,\n",
    ")  # データローダーとデータセットのインポート\n",
    "from tqdm import tqdm  # プログレスバーの表示\n",
    "from transformers import (  # Transformersライブラリからのモデルとトークナイザのインポート\n",
    "    AdamW,\n",
    "    BertForSequenceClassification,\n",
    "    BertModel,\n",
    "    BertTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "df = pd.read_csv(\"training_data_without_nan.tsv\", sep=\"\\t\")\n",
    "\n",
    "# ラベルとテキストの抽出\n",
    "df[\"label\"] = (\n",
    "    df[\"ラベル\"].astype(\"category\").cat.codes\n",
    ")  # ラベルをカテゴリから数値に変換\n",
    "labels = df[\"ラベル\"].astype(\"category\").cat.categories.tolist()  # カテゴリリストを取得\n",
    "df[\"text\"] = df[\"文章\"]  # テキストデータを抽出\n",
    "df[\"satisfaction\"] = df[\"満足度\"]  # 満足度データを抽出\n",
    "\n",
    "# トレーニングデータとテストデータに分割\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)  # データを8:2に分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットクラスの定義\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, df: pd.DataFrame, tokenizer: BertTokenizer, max_length: int = 128\n",
    "    ):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        text = self.df.iloc[idx][\"text\"]\n",
    "        label = self.df.iloc[idx][\"label\"]\n",
    "        satisfaction = self.df.iloc[idx][\"satisfaction\"]\n",
    "\n",
    "        # トークナイズ\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # 辞書形式でデータを返す\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "            \"satisfaction\": torch.tensor(satisfaction, dtype=torch.float),\n",
    "        }\n",
    "\n",
    "\n",
    "# パディングを処理するcollate_fnの定義\n",
    "def collate_fn(batch: list) -> dict:\n",
    "    max_length = max([item[\"input_ids\"].size(0) for item in batch])\n",
    "\n",
    "    input_ids = torch.stack(\n",
    "        [\n",
    "            torch.cat(\n",
    "                [item[\"input_ids\"], torch.zeros(max_length - item[\"input_ids\"].size(0))]\n",
    "            ).long()\n",
    "            for item in batch\n",
    "        ]\n",
    "    )\n",
    "    attention_mask = torch.stack(\n",
    "        [\n",
    "            torch.cat(\n",
    "                [\n",
    "                    item[\"attention_mask\"],\n",
    "                    torch.zeros(max_length - item[\"attention_mask\"].size(0)),\n",
    "                ]\n",
    "            ).long()\n",
    "            for item in batch\n",
    "        ]\n",
    "    )\n",
    "    labels = torch.stack([item[\"label\"] for item in batch])\n",
    "    satisfaction = torch.stack([item[\"satisfaction\"] for item in batch])\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"label\": labels,\n",
    "        \"satisfaction\": satisfaction,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの設定\n",
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "# MODEL_NAME = \"cl-tohoku/bert-large-japanese-v2\"\n",
    "batch_size = 16\n",
    "\n",
    "# トークナイザーとモデルの読み込み\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=len(labels)\n",
    ")\n",
    "\n",
    "# デバイスの設定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# データローダーの作成\n",
    "train_dataset = TextDataset(train_df, tokenizer)\n",
    "test_dataset = TextDataset(test_df, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# オプティマイザと損失関数の定義\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの訓練\n",
    "def train_epoch(\n",
    "    model, data_loader, criterion, optimizer, device\n",
    ") -> tuple[float, float]:\n",
    "    model.train()  # モデルを訓練モードに設定\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)  # 損失の計算\n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()  # 勾配の初期化\n",
    "        loss.backward()  # 逆伝播\n",
    "        optimizer.step()  # オプティマイザのステップ\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), total_loss / len(\n",
    "        data_loader\n",
    "    )\n",
    "\n",
    "\n",
    "# モデルの評価関数\n",
    "def eval_model(model, data_loader, criterion, device) -> tuple[float, float]:\n",
    "    model.eval()  # モデルを評価モードに設定\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.logits, labels)  # 損失の計算\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), total_loss / len(\n",
    "        data_loader\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習前後の精度を比較\n",
    "# 学習前の精度\n",
    "pre_train_accuracy, _ = eval_model(model, test_loader, criterion, device)\n",
    "print(f\"Accuracy before training: {pre_train_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練と評価の実行\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_accuracy, train_loss = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device\n",
    "    )\n",
    "    print(f\"Train loss: {train_loss}, Train accuracy: {train_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習後の精度評価\n",
    "post_train_accuracy, _ = eval_model(model, test_loader, criterion, device)\n",
    "print(f\"Accuracy after training: {post_train_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習済みモデルの保存\n",
    "# torch.save(model.state_dict(), \"model_bert_large_japanese_v2.pth\")\n",
    "# torch.save(model.state_dict(), \"model_bert_base_japanese_v3.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測関数\n",
    "def predict(\n",
    "    text: str,\n",
    "    model: BertForSequenceClassification,\n",
    "    tokenizer: BertTokenizer,\n",
    "    device: torch.device,\n",
    "    top_k: int = 1,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    テキストのカテゴリと満足度を予測する関数\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        予測対象のテキスト\n",
    "    model : BertForSequenceClassification\n",
    "        予測に使用する事前学習済みモデル\n",
    "    tokenizer : BertTokenizer\n",
    "        トークナイザー\n",
    "    device : torch.device\n",
    "        使用するデバイス (CPUまたはGPU)\n",
    "    top_k : int, optional\n",
    "        上位何カテゴリを表示するか (デフォルトは1)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        予測結果 (カテゴリと満足度)\n",
    "    \"\"\"\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        return_token_type_ids=False,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    model.eval()\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits  # 出力ロジットを取得\n",
    "    satisfaction = torch.tanh(\n",
    "        logits[:, -1]\n",
    "    ).squeeze()  # tanh関数を使用して-1から1の範囲に収める\n",
    "    # satisfaction = logits[:, -1]\n",
    "\n",
    "    probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "    top_probs, top_classes = torch.topk(probs, top_k, dim=1)\n",
    "\n",
    "    predictions = []\n",
    "    for i in range(top_k):\n",
    "        predictions.append(\n",
    "            {\n",
    "                \"category\": labels[top_classes[0][i]],\n",
    "                \"confidence\": top_probs[0][i].item(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"predictions\": predictions,\n",
    "        \"satisfaction\": satisfaction.item(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例\n",
    "example_texts = [\n",
    "    \"行きたいところにすぐに行ける\",\n",
    "    \"子供が喜ぶ施設が多い\",\n",
    "    \"食事が美味しい\",\n",
    "    \"自然が多い\",\n",
    "    \"かなり発展した街なのに映画館がない。ライブハウスがない。\",\n",
    "]\n",
    "for example_text in example_texts:\n",
    "    prediction = predict(example_text, model, tokenizer, device)\n",
    "    print(f\"Text: {example_text}\")\n",
    "    print(f\"Prediction Category: {prediction['predictions']}\")\n",
    "    # print(f\"Satisfaction: {prediction['satisfaction']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
