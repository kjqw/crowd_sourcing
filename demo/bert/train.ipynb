{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/transformers/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 必要なライブラリのインポート\n",
    "\n",
    "import numpy as np  # 数値計算用ライブラリ\n",
    "import pandas as pd  # データ操作用ライブラリ\n",
    "import torch  # PyTorchのインポート\n",
    "from sklearn.model_selection import train_test_split  # データ分割のための関数\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    "    Dataset,\n",
    ")  # データローダーとデータセットのインポート\n",
    "from tqdm import tqdm  # プログレスバーの表示\n",
    "from transformers import (  # Transformersライブラリからのモデルとトークナイザのインポート\n",
    "    AdamW,\n",
    "    BertForSequenceClassification,\n",
    "    BertModel,\n",
    "    BertTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "df = pd.read_csv(\"training_data_without_nan.tsv\", sep=\"\\t\")\n",
    "\n",
    "# ラベルとテキストの抽出\n",
    "df[\"label\"] = (\n",
    "    df[\"ラベル\"].astype(\"category\").cat.codes\n",
    ")  # ラベルをカテゴリから数値に変換\n",
    "labels = df[\"ラベル\"].astype(\"category\").cat.categories.tolist()  # カテゴリリストを取得\n",
    "df[\"text\"] = df[\"文章\"]  # テキストデータを抽出\n",
    "df[\"satisfaction\"] = df[\"満足度\"]  # 満足度データを抽出\n",
    "\n",
    "# トレーニングデータとテストデータに分割\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)  # データを8:2に分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットクラスの定義\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, df: pd.DataFrame, tokenizer: BertTokenizer, max_length: int = 128\n",
    "    ):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        text = self.df.iloc[idx][\"text\"]\n",
    "        label = self.df.iloc[idx][\"label\"]\n",
    "        satisfaction = self.df.iloc[idx][\"satisfaction\"]\n",
    "\n",
    "        # トークナイズ\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # 辞書形式でデータを返す\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "            \"satisfaction\": torch.tensor(satisfaction, dtype=torch.float),\n",
    "        }\n",
    "\n",
    "\n",
    "# パディングを処理するcollate_fnの定義\n",
    "def collate_fn(batch: list) -> dict:\n",
    "    max_length = max([item[\"input_ids\"].size(0) for item in batch])\n",
    "\n",
    "    input_ids = torch.stack(\n",
    "        [\n",
    "            torch.cat(\n",
    "                [item[\"input_ids\"], torch.zeros(max_length - item[\"input_ids\"].size(0))]\n",
    "            ).long()\n",
    "            for item in batch\n",
    "        ]\n",
    "    )\n",
    "    attention_mask = torch.stack(\n",
    "        [\n",
    "            torch.cat(\n",
    "                [\n",
    "                    item[\"attention_mask\"],\n",
    "                    torch.zeros(max_length - item[\"attention_mask\"].size(0)),\n",
    "                ]\n",
    "            ).long()\n",
    "            for item in batch\n",
    "        ]\n",
    "    )\n",
    "    labels = torch.stack([item[\"label\"] for item in batch])\n",
    "    satisfaction = torch.stack([item[\"satisfaction\"] for item in batch])\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"label\": labels,\n",
    "        \"satisfaction\": satisfaction,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/envs/transformers/lib/python3.12/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# モデルの設定\n",
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "# MODEL_NAME = \"cl-tohoku/bert-large-japanese-v2\"\n",
    "batch_size = 16\n",
    "\n",
    "# トークナイザーとモデルの読み込み\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=len(labels)\n",
    ")\n",
    "\n",
    "# デバイスの設定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# データローダーの作成\n",
    "train_dataset = TextDataset(train_df, tokenizer)\n",
    "test_dataset = TextDataset(test_df, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# オプティマイザと損失関数の定義\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの訓練\n",
    "def train_epoch(\n",
    "    model, data_loader, criterion, optimizer, device\n",
    ") -> tuple[float, float]:\n",
    "    model.train()  # モデルを訓練モードに設定\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)  # 損失の計算\n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()  # 勾配の初期化\n",
    "        loss.backward()  # 逆伝播\n",
    "        optimizer.step()  # オプティマイザのステップ\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), total_loss / len(\n",
    "        data_loader\n",
    "    )\n",
    "\n",
    "\n",
    "# モデルの評価関数\n",
    "def eval_model(model, data_loader, criterion, device) -> tuple[float, float]:\n",
    "    model.eval()  # モデルを評価モードに設定\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.logits, labels)  # 損失の計算\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), total_loss / len(\n",
    "        data_loader\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 25/25 [00:01<00:00, 18.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before training: 0.12658227848101267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 学習前後の精度を比較\n",
    "# 学習前の精度\n",
    "pre_train_accuracy, _ = eval_model(model, test_loader, criterion, device)\n",
    "print(f\"Accuracy before training: {pre_train_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 99/99 [00:15<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.200716941043584, Train accuracy: 0.3688212927756654\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 99/99 [00:15<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.3217129502633604, Train accuracy: 0.6501901140684411\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 99/99 [00:15<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.9314218341100096, Train accuracy: 0.7477820025348542\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 99/99 [00:15<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7121679388513469, Train accuracy: 0.806083650190114\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 99/99 [00:15<00:00,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5485909630854925, Train accuracy: 0.8510773130544993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 訓練と評価の実行\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_accuracy, train_loss = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device\n",
    "    )\n",
    "    print(f\"Train loss: {train_loss}, Train accuracy: {train_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 25/25 [00:01<00:00, 20.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after training: 0.6430379746835443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 学習後の精度評価\n",
    "post_train_accuracy, _ = eval_model(model, test_loader, criterion, device)\n",
    "print(f\"Accuracy after training: {post_train_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習済みモデルの保存\n",
    "# torch.save(model.state_dict(), \"model_bert_large_japanese_v2.pth\")\n",
    "# torch.save(model.state_dict(), \"model_bert_base_japanese_v3.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測関数\n",
    "def predict(\n",
    "    text: str,\n",
    "    model: BertForSequenceClassification,\n",
    "    tokenizer: BertTokenizer,\n",
    "    device: torch.device,\n",
    "    top_k: int = 1,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    テキストのカテゴリと満足度を予測する関数\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        予測対象のテキスト\n",
    "    model : BertForSequenceClassification\n",
    "        予測に使用する事前学習済みモデル\n",
    "    tokenizer : BertTokenizer\n",
    "        トークナイザー\n",
    "    device : torch.device\n",
    "        使用するデバイス (CPUまたはGPU)\n",
    "    top_k : int, optional\n",
    "        上位何カテゴリを表示するか (デフォルトは1)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        予測結果 (カテゴリと満足度)\n",
    "    \"\"\"\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        return_token_type_ids=False,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    model.eval()\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits  # 出力ロジットを取得\n",
    "    satisfaction = torch.tanh(\n",
    "        logits[:, -1]\n",
    "    ).squeeze()  # tanh関数を使用して-1から1の範囲に収める\n",
    "    # satisfaction = logits[:, -1]\n",
    "\n",
    "    probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "    top_probs, top_classes = torch.topk(probs, top_k, dim=1)\n",
    "\n",
    "    predictions = []\n",
    "    for i in range(top_k):\n",
    "        predictions.append(\n",
    "            {\n",
    "                \"category\": labels[top_classes[0][i]],\n",
    "                \"confidence\": top_probs[0][i].item(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"predictions\": predictions,\n",
    "        \"satisfaction\": satisfaction.item(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 行きたいところにすぐに行ける\n",
      "Prediction Category: [{'category': '移動・交通', 'confidence': 0.9709092378616333}]\n",
      "\n",
      "Text: 子供が喜ぶ施設が多い\n",
      "Prediction Category: [{'category': '医療・福祉', 'confidence': 0.5797423124313354}]\n",
      "\n",
      "Text: 食事が美味しい\n",
      "Prediction Category: [{'category': '買物・飲食', 'confidence': 0.3605135977268219}]\n",
      "\n",
      "Text: 自然が多い\n",
      "Prediction Category: [{'category': '自然景観', 'confidence': 0.6363059878349304}]\n",
      "\n",
      "Text: かなり発展した街なのに映画館がない。ライブハウスがない。\n",
      "Prediction Category: [{'category': '遊び・娯楽', 'confidence': 0.9026836156845093}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 例\n",
    "example_texts = [\n",
    "    \"行きたいところにすぐに行ける\",\n",
    "    \"子供が喜ぶ施設が多い\",\n",
    "    \"食事が美味しい\",\n",
    "    \"自然が多い\",\n",
    "    \"かなり発展した街なのに映画館がない。ライブハウスがない。\",\n",
    "]\n",
    "for example_text in example_texts:\n",
    "    prediction = predict(example_text, model, tokenizer, device)\n",
    "    print(f\"Text: {example_text}\")\n",
    "    print(f\"Prediction Category: {prediction['predictions']}\")\n",
    "    # print(f\"Satisfaction: {prediction['satisfaction']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
